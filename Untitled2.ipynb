{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMU6B/AwC1AS364r5x13H0Z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1c8e77966b0849b894daecbdf3959d71": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_b59c7f8a45244e098275cc0b62a947fd",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_1a064594a1024a73a6fda04a37eaf22b",
              "IPY_MODEL_324958b2dedb4e95b1e9e9cf27017ea3",
              "IPY_MODEL_f9a51471ac3f4437b0e21cf3f81cc5e2"
            ]
          }
        },
        "b59c7f8a45244e098275cc0b62a947fd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1a064594a1024a73a6fda04a37eaf22b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_871f263769ad4aec8c58fafccc342129",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_48950fb206f847d69bf372c390192dbc"
          }
        },
        "324958b2dedb4e95b1e9e9cf27017ea3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_a061f9b43bf84e299c3bacc041037fe1",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 248100043,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 248100043,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_47482b97360b476690de1c28f4153d5c"
          }
        },
        "f9a51471ac3f4437b0e21cf3f81cc5e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_2c84d3fd1fad41f4b585b4f7f69bbace",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 248100864/? [00:08&lt;00:00, 50653335.86it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_06105fa26bab4c31aac2e5f96f06e194"
          }
        },
        "871f263769ad4aec8c58fafccc342129": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "48950fb206f847d69bf372c390192dbc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a061f9b43bf84e299c3bacc041037fe1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "47482b97360b476690de1c28f4153d5c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2c84d3fd1fad41f4b585b4f7f69bbace": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "06105fa26bab4c31aac2e5f96f06e194": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/utkarshpandey6/Project-Report-1-/blob/main/Untitled2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZV-jxN8Srcer"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class PatchEmbed(nn.Module):\n",
        "  '''\n",
        "    img_size : int\n",
        "      Size of the image\n",
        "\n",
        "    patch_size : int \n",
        "      Size of the patch\n",
        "\n",
        "    in_chans: int \n",
        "      Number of input channels\n",
        "\n",
        "    embed_dim : int\n",
        "      The emmbedding dimension\n",
        "\n",
        "  Attributes\n",
        "\n",
        "  n_patches: int \n",
        "    Number of ptched inside of our image\n",
        "\n",
        "  proj: nn.Conv2d\n",
        "    Convolutional layer that does both the splitting into patched and theri embedding\n",
        "  '''\n",
        "\n",
        "  def __init__(self, img_size, patch_size, in_chans=3, embed_dim=768):\n",
        "    super().__init__()\n",
        "    self.img_size = img_size\n",
        "    self.patch_size = patch_size\n",
        "    self.n_patches = (img_size // patch_size) ** 2\n",
        "    self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
        "\n",
        "  def forward(self, x):\n",
        "    '''\n",
        "      parameters\n",
        "        shape (n_samples, in_chans, img_size, img_size)`\n",
        "      returns\n",
        "        shape (n_samples, n_patches, embed_dim)\n",
        "    '''\n",
        "\n",
        "    x = self.proj(x) # (n_samples, embed_dim, n_patches ** 0.5, n_patches ** 0.5)\n",
        "    x = x.flatten(2)  # shape (n_samples, embed_dim,  n_patches) \n",
        "    x = x.transpose(1, 2) # shape (n_samples, n_patches, embed_dim)\n",
        "\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Attention(nn.Module):\n",
        "    \"\"\"Attention mechanism.\n",
        "    Parameters\n",
        "    ----------\n",
        "    dim : int\n",
        "        The input and out dimension of per token features.\n",
        "    n_heads : int\n",
        "        Number of attention heads.\n",
        "    qkv_bias : bool\n",
        "        If True then we include bias to the query, key and value projections.\n",
        "    attn_p : float\n",
        "        Dropout probability applied to the query, key and value tensors.\n",
        "    proj_p : float\n",
        "        Dropout probability applied to the output tensor.\n",
        "    Attributes\n",
        "    ----------\n",
        "    scale : float\n",
        "        Normalizing consant for the dot product.\n",
        "    qkv : nn.Linear\n",
        "        Linear projection for the query, key and value.\n",
        "    proj : nn.Linear\n",
        "        Linear mapping that takes in the concatenated output of all attention\n",
        "        heads and maps it into a new space.\n",
        "    attn_drop, proj_drop : nn.Dropout\n",
        "        Dropout layers.\n",
        "    \"\"\"\n",
        "    def __init__(self, dim, n_heads=12, qkv_bias=True, attn_p=0., proj_p=0.):\n",
        "        super().__init__()\n",
        "        self.n_heads = n_heads\n",
        "        self.dim = dim\n",
        "        self.head_dim = dim // n_heads\n",
        "        self.scale = self.head_dim ** -0.5\n",
        "\n",
        "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
        "        self.attn_drop = nn.Dropout(attn_p)\n",
        "        self.proj = nn.Linear(dim, dim)\n",
        "        self.proj_drop = nn.Dropout(proj_p)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Run forward pass.\n",
        "        Parameters\n",
        "        ----------\n",
        "        x : torch.Tensor\n",
        "            Shape `(n_samples, n_patches + 1, dim)`.\n",
        "        Returns\n",
        "        -------\n",
        "        torch.Tensor\n",
        "            Shape `(n_samples, n_patches + 1, dim)`.\n",
        "        \"\"\"\n",
        "        n_samples, n_tokens, dim = x.shape\n",
        "\n",
        "        if dim != self.dim:\n",
        "            raise ValueError\n",
        "\n",
        "        qkv = self.qkv(x)  # (n_samples, n_patches + 1, 3 * dim)\n",
        "        qkv = qkv.reshape(\n",
        "                n_samples, n_tokens, 3, self.n_heads, self.head_dim\n",
        "        )  # (n_smaples, n_patches + 1, 3, n_heads, head_dim)\n",
        "        qkv = qkv.permute(\n",
        "                2, 0, 3, 1, 4\n",
        "        )  # (3, n_samples, n_heads, n_patches + 1, head_dim)\n",
        "\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
        "        k_t = k.transpose(-2, -1)  # (n_samples, n_heads, head_dim, n_patches + 1)\n",
        "        dp = (\n",
        "           q @ k_t\n",
        "        ) * self.scale # (n_samples, n_heads, n_patches + 1, n_patches + 1)\n",
        "        attn = dp.softmax(dim=-1)  # (n_samples, n_heads, n_patches + 1, n_patches + 1)\n",
        "        attn = self.attn_drop(attn)\n",
        "\n",
        "        weighted_avg = attn @ v  # (n_samples, n_heads, n_patches +1, head_dim)\n",
        "        weighted_avg = weighted_avg.transpose(\n",
        "                1, 2\n",
        "        )  # (n_samples, n_patches + 1, n_heads, head_dim)\n",
        "        weighted_avg = weighted_avg.flatten(2)  # (n_samples, n_patches + 1, dim)\n",
        "\n",
        "        x = self.proj(weighted_avg)  # (n_samples, n_patches + 1, dim)\n",
        "        x = self.proj_drop(x)  # (n_samples, n_patches + 1, dim)\n",
        "\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "igQb2tRuum8z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "    \"\"\"Multilayer perceptron.\n",
        "    Parameters\n",
        "    ----------\n",
        "    in_features : int\n",
        "        Number of input features.\n",
        "    hidden_features : int\n",
        "        Number of nodes in the hidden layer.\n",
        "    out_features : int\n",
        "        Number of output features.\n",
        "    p : float\n",
        "        Dropout probability.\n",
        "    Attributes\n",
        "    ----------\n",
        "    fc : nn.Linear\n",
        "        The First linear layer.\n",
        "    act : nn.GELU\n",
        "        GELU activation function.\n",
        "    fc2 : nn.Linear\n",
        "        The second linear layer.\n",
        "    drop : nn.Dropout\n",
        "        Dropout layer.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_features, hidden_features, out_features, p=0.):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
        "        self.act = nn.GELU()\n",
        "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
        "        self.drop = nn.Dropout(p)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Run forward pass.\n",
        "        Parameters\n",
        "        ----------\n",
        "        x : torch.Tensor\n",
        "            Shape `(n_samples, n_patches + 1, in_features)`.\n",
        "        Returns\n",
        "        -------\n",
        "        torch.Tensor\n",
        "            Shape `(n_samples, n_patches +1, out_features)`\n",
        "        \"\"\"\n",
        "        x = self.fc1(\n",
        "                x\n",
        "        ) # (n_samples, n_patches + 1, hidden_features)\n",
        "        x = self.act(x)  # (n_samples, n_patches + 1, hidden_features)\n",
        "        x = self.drop(x)  # (n_samples, n_patches + 1, hidden_features)\n",
        "        x = self.fc2(x)  # (n_samples, n_patches + 1, hidden_features)\n",
        "        x = self.drop(x)  # (n_samples, n_patches + 1, hidden_features)\n",
        "\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "tMXcRZ3uxoIt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\"Transformer block.\n",
        "    Parameters\n",
        "    ----------\n",
        "    dim : int\n",
        "        Embeddinig dimension.\n",
        "    n_heads : int\n",
        "        Number of attention heads.\n",
        "    mlp_ratio : float\n",
        "        Determines the hidden dimension size of the `MLP` module with respect\n",
        "        to `dim`.\n",
        "    qkv_bias : bool\n",
        "        If True then we include bias to the query, key and value projections.\n",
        "    p, attn_p : float\n",
        "        Dropout probability.\n",
        "    Attributes\n",
        "    ----------\n",
        "    norm1, norm2 : LayerNorm\n",
        "        Layer normalization.\n",
        "    attn : Attention\n",
        "        Attention module.\n",
        "    mlp : MLP\n",
        "        MLP module.\n",
        "    \"\"\"\n",
        "    def __init__(self, dim, n_heads, mlp_ratio=4.0, qkv_bias=True, p=0., attn_p=0.):\n",
        "        super().__init__()\n",
        "        self.norm1 = nn.LayerNorm(dim, eps=1e-6)\n",
        "        self.attn = Attention(\n",
        "                dim,\n",
        "                n_heads=n_heads,\n",
        "                qkv_bias=qkv_bias,\n",
        "                attn_p=attn_p,\n",
        "                proj_p=p\n",
        "        )\n",
        "        self.norm2 = nn.LayerNorm(dim, eps=1e-6)\n",
        "        hidden_features = int(dim * mlp_ratio)\n",
        "        self.mlp = MLP(\n",
        "                in_features=dim,\n",
        "                hidden_features=hidden_features,\n",
        "                out_features=dim,\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Run forward pass.\n",
        "        Parameters\n",
        "        ----------\n",
        "        x : torch.Tensor\n",
        "            Shape `(n_samples, n_patches + 1, dim)`.\n",
        "        Returns\n",
        "        -------\n",
        "        torch.Tensor\n",
        "            Shape `(n_samples, n_patches + 1, dim)`.\n",
        "        \"\"\"\n",
        "        x = x + self.attn(self.norm1(x))\n",
        "        x = x + self.mlp(self.norm2(x))\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "rppLGBrtx2KO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VisionTransformer(nn.Module):\n",
        "    \"\"\"Simplified implementation of the Vision transformer.\n",
        "    Parameters\n",
        "    ----------\n",
        "    img_size : int\n",
        "        Both height and the width of the image (it is a square).\n",
        "    patch_size : int\n",
        "        Both height and the width of the patch (it is a square).\n",
        "    in_chans : int\n",
        "        Number of input channels.\n",
        "    n_classes : int\n",
        "        Number of classes.\n",
        "    embed_dim : int\n",
        "        Dimensionality of the token/patch embeddings.\n",
        "    depth : int\n",
        "        Number of blocks.\n",
        "    n_heads : int\n",
        "        Number of attention heads.\n",
        "    mlp_ratio : float\n",
        "        Determines the hidden dimension of the `MLP` module.\n",
        "    qkv_bias : bool\n",
        "        If True then we include bias to the query, key and value projections.\n",
        "    p, attn_p : float\n",
        "        Dropout probability.\n",
        "    Attributes\n",
        "    ----------\n",
        "    patch_embed : PatchEmbed\n",
        "        Instance of `PatchEmbed` layer.\n",
        "    cls_token : nn.Parameter\n",
        "        Learnable parameter that will represent the first token in the sequence.\n",
        "        It has `embed_dim` elements.\n",
        "    pos_emb : nn.Parameter\n",
        "        Positional embedding of the cls token + all the patches.\n",
        "        It has `(n_patches + 1) * embed_dim` elements.\n",
        "    pos_drop : nn.Dropout\n",
        "        Dropout layer.\n",
        "    blocks : nn.ModuleList\n",
        "        List of `Block` modules.\n",
        "    norm : nn.LayerNorm\n",
        "        Layer normalization.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "            self,\n",
        "            img_size=384,\n",
        "            patch_size=16,\n",
        "            in_chans=3,\n",
        "            n_classes=1000,\n",
        "            embed_dim=768,\n",
        "            depth=12,\n",
        "            n_heads=12,\n",
        "            mlp_ratio=4.,\n",
        "            qkv_bias=True,\n",
        "            p=0.,\n",
        "            attn_p=0.,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.patch_embed = PatchEmbed(\n",
        "                img_size=img_size,\n",
        "                patch_size=patch_size,\n",
        "                in_chans=in_chans,\n",
        "                embed_dim=embed_dim,\n",
        "        )\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
        "        self.pos_embed = nn.Parameter(\n",
        "                torch.zeros(1, 1 + self.patch_embed.n_patches, embed_dim)\n",
        "        )\n",
        "        self.pos_drop = nn.Dropout(p=p)\n",
        "\n",
        "        self.blocks = nn.ModuleList(\n",
        "            [\n",
        "                Block(\n",
        "                    dim=embed_dim,\n",
        "                    n_heads=n_heads,\n",
        "                    mlp_ratio=mlp_ratio,\n",
        "                    qkv_bias=qkv_bias,\n",
        "                    p=p,\n",
        "                    attn_p=attn_p,\n",
        "                )\n",
        "                for _ in range(depth)\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        self.norm = nn.LayerNorm(embed_dim, eps=1e-6)\n",
        "        self.head = nn.Linear(embed_dim, n_classes)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Run the forward pass.\n",
        "        Parameters\n",
        "        ----------\n",
        "        x : torch.Tensor\n",
        "            Shape `(n_samples, in_chans, img_size, img_size)`.\n",
        "        Returns\n",
        "        -------\n",
        "        logits : torch.Tensor\n",
        "            Logits over all the classes - `(n_samples, n_classes)`.\n",
        "        \"\"\"\n",
        "        n_samples = x.shape[0]\n",
        "        x = self.patch_embed(x)\n",
        "\n",
        "        cls_token = self.cls_token.expand(\n",
        "                n_samples, -1, -1\n",
        "        )  # (n_samples, 1, embed_dim)\n",
        "        x = torch.cat((cls_token, x), dim=1)  # (n_samples, 1 + n_patches, embed_dim)\n",
        "        x = x + self.pos_embed  # (n_samples, 1 + n_patches, embed_dim)\n",
        "        x = self.pos_drop(x)\n",
        "\n",
        "        for block in self.blocks:\n",
        "            x = block(x)\n",
        "\n",
        "        x = self.norm(x)\n",
        "\n",
        "        cls_token_final = x[:, 0]  # just the CLS token\n",
        "        x = self.head(cls_token_final)\n",
        "\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "Yp8ZqdBoy2Op"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install timm"
      ],
      "metadata": {
        "id": "t7ux9BRr1Vo1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9af4cd9c-f59f-486d-e940-cd4c089e1806"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting timm\n",
            "  Downloading timm-0.4.12-py3-none-any.whl (376 kB)\n",
            "\u001b[?25l\r\u001b[K     |▉                               | 10 kB 28.2 MB/s eta 0:00:01\r\u001b[K     |█▊                              | 20 kB 22.5 MB/s eta 0:00:01\r\u001b[K     |██▋                             | 30 kB 11.4 MB/s eta 0:00:01\r\u001b[K     |███▌                            | 40 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |████▍                           | 51 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 61 kB 5.8 MB/s eta 0:00:01\r\u001b[K     |██████                          | 71 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |███████                         | 81 kB 6.2 MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 92 kB 4.8 MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 102 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 112 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 122 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 133 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 143 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 153 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 163 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 174 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 184 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 194 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 204 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 215 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 225 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 235 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 245 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 256 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 266 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 276 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 286 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 296 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 307 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 317 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 327 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 337 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 348 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 358 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 368 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 376 kB 5.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from timm) (0.11.1+cu111)\n",
            "Requirement already satisfied: torch>=1.4 in /usr/local/lib/python3.7/dist-packages (from timm) (1.10.0+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.4->timm) (3.10.0.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision->timm) (1.19.5)\n",
            "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->timm) (7.1.2)\n",
            "Installing collected packages: timm\n",
            "Successfully installed timm-0.4.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import timm\n",
        "import torch\n",
        "\n",
        "\n",
        "# Helpers\n",
        "def get_n_params(module):\n",
        "    return sum(p.numel() for p in module.parameters() if p.requires_grad)\n",
        "\n",
        "def assert_tensors_equal(t1, t2):\n",
        "    a1, a2 = t1.detach().numpy(), t2.detach().numpy()\n",
        "\n",
        "    np.testing.assert_allclose(a1, a2)\n",
        "\n",
        "model_name = \"vit_base_patch16_384\"\n",
        "model_official = timm.create_model(model_name, pretrained=True)\n",
        "model_official.eval()\n",
        "print(type(model_official))\n",
        "\n",
        "custom_config = {\n",
        "        \"img_size\": 384,\n",
        "        \"in_chans\": 3,\n",
        "        \"patch_size\": 16,\n",
        "        \"embed_dim\": 768,\n",
        "        \"depth\": 12,\n",
        "        \"n_heads\": 12,\n",
        "        \"qkv_bias\": True,\n",
        "        \"mlp_ratio\": 4,\n",
        "}\n",
        "\n",
        "model_custom = VisionTransformer(**custom_config)\n",
        "model_custom.eval()\n",
        "\n",
        "\n",
        "for (n_o, p_o), (n_c, p_c) in zip(\n",
        "        model_official.named_parameters(), model_custom.named_parameters()\n",
        "):\n",
        "    assert p_o.numel() == p_c.numel()\n",
        "    print(f\"{n_o} | {n_c}\")\n",
        "\n",
        "    p_c.data[:] = p_o.data\n",
        "\n",
        "    assert_tensors_equal(p_c.data, p_o.data)\n",
        "\n",
        "inp = torch.rand(1, 3, 384, 384)\n",
        "res_c = model_custom(inp)\n",
        "res_o = model_official(inp)\n",
        "\n",
        "# Asserts\n",
        "assert get_n_params(model_custom) == get_n_params(model_official)\n",
        "assert_tensors_equal(res_c, res_o)"
      ],
      "metadata": {
        "id": "Bh5aTe3Y0t6s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c0eab32-f589-4207-93e6-9575829f0599"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'timm.models.vision_transformer.VisionTransformer'>\n",
            "cls_token | cls_token\n",
            "pos_embed | pos_embed\n",
            "patch_embed.proj.weight | patch_embed.proj.weight\n",
            "patch_embed.proj.bias | patch_embed.proj.bias\n",
            "blocks.0.norm1.weight | blocks.0.norm1.weight\n",
            "blocks.0.norm1.bias | blocks.0.norm1.bias\n",
            "blocks.0.attn.qkv.weight | blocks.0.attn.qkv.weight\n",
            "blocks.0.attn.qkv.bias | blocks.0.attn.qkv.bias\n",
            "blocks.0.attn.proj.weight | blocks.0.attn.proj.weight\n",
            "blocks.0.attn.proj.bias | blocks.0.attn.proj.bias\n",
            "blocks.0.norm2.weight | blocks.0.norm2.weight\n",
            "blocks.0.norm2.bias | blocks.0.norm2.bias\n",
            "blocks.0.mlp.fc1.weight | blocks.0.mlp.fc1.weight\n",
            "blocks.0.mlp.fc1.bias | blocks.0.mlp.fc1.bias\n",
            "blocks.0.mlp.fc2.weight | blocks.0.mlp.fc2.weight\n",
            "blocks.0.mlp.fc2.bias | blocks.0.mlp.fc2.bias\n",
            "blocks.1.norm1.weight | blocks.1.norm1.weight\n",
            "blocks.1.norm1.bias | blocks.1.norm1.bias\n",
            "blocks.1.attn.qkv.weight | blocks.1.attn.qkv.weight\n",
            "blocks.1.attn.qkv.bias | blocks.1.attn.qkv.bias\n",
            "blocks.1.attn.proj.weight | blocks.1.attn.proj.weight\n",
            "blocks.1.attn.proj.bias | blocks.1.attn.proj.bias\n",
            "blocks.1.norm2.weight | blocks.1.norm2.weight\n",
            "blocks.1.norm2.bias | blocks.1.norm2.bias\n",
            "blocks.1.mlp.fc1.weight | blocks.1.mlp.fc1.weight\n",
            "blocks.1.mlp.fc1.bias | blocks.1.mlp.fc1.bias\n",
            "blocks.1.mlp.fc2.weight | blocks.1.mlp.fc2.weight\n",
            "blocks.1.mlp.fc2.bias | blocks.1.mlp.fc2.bias\n",
            "blocks.2.norm1.weight | blocks.2.norm1.weight\n",
            "blocks.2.norm1.bias | blocks.2.norm1.bias\n",
            "blocks.2.attn.qkv.weight | blocks.2.attn.qkv.weight\n",
            "blocks.2.attn.qkv.bias | blocks.2.attn.qkv.bias\n",
            "blocks.2.attn.proj.weight | blocks.2.attn.proj.weight\n",
            "blocks.2.attn.proj.bias | blocks.2.attn.proj.bias\n",
            "blocks.2.norm2.weight | blocks.2.norm2.weight\n",
            "blocks.2.norm2.bias | blocks.2.norm2.bias\n",
            "blocks.2.mlp.fc1.weight | blocks.2.mlp.fc1.weight\n",
            "blocks.2.mlp.fc1.bias | blocks.2.mlp.fc1.bias\n",
            "blocks.2.mlp.fc2.weight | blocks.2.mlp.fc2.weight\n",
            "blocks.2.mlp.fc2.bias | blocks.2.mlp.fc2.bias\n",
            "blocks.3.norm1.weight | blocks.3.norm1.weight\n",
            "blocks.3.norm1.bias | blocks.3.norm1.bias\n",
            "blocks.3.attn.qkv.weight | blocks.3.attn.qkv.weight\n",
            "blocks.3.attn.qkv.bias | blocks.3.attn.qkv.bias\n",
            "blocks.3.attn.proj.weight | blocks.3.attn.proj.weight\n",
            "blocks.3.attn.proj.bias | blocks.3.attn.proj.bias\n",
            "blocks.3.norm2.weight | blocks.3.norm2.weight\n",
            "blocks.3.norm2.bias | blocks.3.norm2.bias\n",
            "blocks.3.mlp.fc1.weight | blocks.3.mlp.fc1.weight\n",
            "blocks.3.mlp.fc1.bias | blocks.3.mlp.fc1.bias\n",
            "blocks.3.mlp.fc2.weight | blocks.3.mlp.fc2.weight\n",
            "blocks.3.mlp.fc2.bias | blocks.3.mlp.fc2.bias\n",
            "blocks.4.norm1.weight | blocks.4.norm1.weight\n",
            "blocks.4.norm1.bias | blocks.4.norm1.bias\n",
            "blocks.4.attn.qkv.weight | blocks.4.attn.qkv.weight\n",
            "blocks.4.attn.qkv.bias | blocks.4.attn.qkv.bias\n",
            "blocks.4.attn.proj.weight | blocks.4.attn.proj.weight\n",
            "blocks.4.attn.proj.bias | blocks.4.attn.proj.bias\n",
            "blocks.4.norm2.weight | blocks.4.norm2.weight\n",
            "blocks.4.norm2.bias | blocks.4.norm2.bias\n",
            "blocks.4.mlp.fc1.weight | blocks.4.mlp.fc1.weight\n",
            "blocks.4.mlp.fc1.bias | blocks.4.mlp.fc1.bias\n",
            "blocks.4.mlp.fc2.weight | blocks.4.mlp.fc2.weight\n",
            "blocks.4.mlp.fc2.bias | blocks.4.mlp.fc2.bias\n",
            "blocks.5.norm1.weight | blocks.5.norm1.weight\n",
            "blocks.5.norm1.bias | blocks.5.norm1.bias\n",
            "blocks.5.attn.qkv.weight | blocks.5.attn.qkv.weight\n",
            "blocks.5.attn.qkv.bias | blocks.5.attn.qkv.bias\n",
            "blocks.5.attn.proj.weight | blocks.5.attn.proj.weight\n",
            "blocks.5.attn.proj.bias | blocks.5.attn.proj.bias\n",
            "blocks.5.norm2.weight | blocks.5.norm2.weight\n",
            "blocks.5.norm2.bias | blocks.5.norm2.bias\n",
            "blocks.5.mlp.fc1.weight | blocks.5.mlp.fc1.weight\n",
            "blocks.5.mlp.fc1.bias | blocks.5.mlp.fc1.bias\n",
            "blocks.5.mlp.fc2.weight | blocks.5.mlp.fc2.weight\n",
            "blocks.5.mlp.fc2.bias | blocks.5.mlp.fc2.bias\n",
            "blocks.6.norm1.weight | blocks.6.norm1.weight\n",
            "blocks.6.norm1.bias | blocks.6.norm1.bias\n",
            "blocks.6.attn.qkv.weight | blocks.6.attn.qkv.weight\n",
            "blocks.6.attn.qkv.bias | blocks.6.attn.qkv.bias\n",
            "blocks.6.attn.proj.weight | blocks.6.attn.proj.weight\n",
            "blocks.6.attn.proj.bias | blocks.6.attn.proj.bias\n",
            "blocks.6.norm2.weight | blocks.6.norm2.weight\n",
            "blocks.6.norm2.bias | blocks.6.norm2.bias\n",
            "blocks.6.mlp.fc1.weight | blocks.6.mlp.fc1.weight\n",
            "blocks.6.mlp.fc1.bias | blocks.6.mlp.fc1.bias\n",
            "blocks.6.mlp.fc2.weight | blocks.6.mlp.fc2.weight\n",
            "blocks.6.mlp.fc2.bias | blocks.6.mlp.fc2.bias\n",
            "blocks.7.norm1.weight | blocks.7.norm1.weight\n",
            "blocks.7.norm1.bias | blocks.7.norm1.bias\n",
            "blocks.7.attn.qkv.weight | blocks.7.attn.qkv.weight\n",
            "blocks.7.attn.qkv.bias | blocks.7.attn.qkv.bias\n",
            "blocks.7.attn.proj.weight | blocks.7.attn.proj.weight\n",
            "blocks.7.attn.proj.bias | blocks.7.attn.proj.bias\n",
            "blocks.7.norm2.weight | blocks.7.norm2.weight\n",
            "blocks.7.norm2.bias | blocks.7.norm2.bias\n",
            "blocks.7.mlp.fc1.weight | blocks.7.mlp.fc1.weight\n",
            "blocks.7.mlp.fc1.bias | blocks.7.mlp.fc1.bias\n",
            "blocks.7.mlp.fc2.weight | blocks.7.mlp.fc2.weight\n",
            "blocks.7.mlp.fc2.bias | blocks.7.mlp.fc2.bias\n",
            "blocks.8.norm1.weight | blocks.8.norm1.weight\n",
            "blocks.8.norm1.bias | blocks.8.norm1.bias\n",
            "blocks.8.attn.qkv.weight | blocks.8.attn.qkv.weight\n",
            "blocks.8.attn.qkv.bias | blocks.8.attn.qkv.bias\n",
            "blocks.8.attn.proj.weight | blocks.8.attn.proj.weight\n",
            "blocks.8.attn.proj.bias | blocks.8.attn.proj.bias\n",
            "blocks.8.norm2.weight | blocks.8.norm2.weight\n",
            "blocks.8.norm2.bias | blocks.8.norm2.bias\n",
            "blocks.8.mlp.fc1.weight | blocks.8.mlp.fc1.weight\n",
            "blocks.8.mlp.fc1.bias | blocks.8.mlp.fc1.bias\n",
            "blocks.8.mlp.fc2.weight | blocks.8.mlp.fc2.weight\n",
            "blocks.8.mlp.fc2.bias | blocks.8.mlp.fc2.bias\n",
            "blocks.9.norm1.weight | blocks.9.norm1.weight\n",
            "blocks.9.norm1.bias | blocks.9.norm1.bias\n",
            "blocks.9.attn.qkv.weight | blocks.9.attn.qkv.weight\n",
            "blocks.9.attn.qkv.bias | blocks.9.attn.qkv.bias\n",
            "blocks.9.attn.proj.weight | blocks.9.attn.proj.weight\n",
            "blocks.9.attn.proj.bias | blocks.9.attn.proj.bias\n",
            "blocks.9.norm2.weight | blocks.9.norm2.weight\n",
            "blocks.9.norm2.bias | blocks.9.norm2.bias\n",
            "blocks.9.mlp.fc1.weight | blocks.9.mlp.fc1.weight\n",
            "blocks.9.mlp.fc1.bias | blocks.9.mlp.fc1.bias\n",
            "blocks.9.mlp.fc2.weight | blocks.9.mlp.fc2.weight\n",
            "blocks.9.mlp.fc2.bias | blocks.9.mlp.fc2.bias\n",
            "blocks.10.norm1.weight | blocks.10.norm1.weight\n",
            "blocks.10.norm1.bias | blocks.10.norm1.bias\n",
            "blocks.10.attn.qkv.weight | blocks.10.attn.qkv.weight\n",
            "blocks.10.attn.qkv.bias | blocks.10.attn.qkv.bias\n",
            "blocks.10.attn.proj.weight | blocks.10.attn.proj.weight\n",
            "blocks.10.attn.proj.bias | blocks.10.attn.proj.bias\n",
            "blocks.10.norm2.weight | blocks.10.norm2.weight\n",
            "blocks.10.norm2.bias | blocks.10.norm2.bias\n",
            "blocks.10.mlp.fc1.weight | blocks.10.mlp.fc1.weight\n",
            "blocks.10.mlp.fc1.bias | blocks.10.mlp.fc1.bias\n",
            "blocks.10.mlp.fc2.weight | blocks.10.mlp.fc2.weight\n",
            "blocks.10.mlp.fc2.bias | blocks.10.mlp.fc2.bias\n",
            "blocks.11.norm1.weight | blocks.11.norm1.weight\n",
            "blocks.11.norm1.bias | blocks.11.norm1.bias\n",
            "blocks.11.attn.qkv.weight | blocks.11.attn.qkv.weight\n",
            "blocks.11.attn.qkv.bias | blocks.11.attn.qkv.bias\n",
            "blocks.11.attn.proj.weight | blocks.11.attn.proj.weight\n",
            "blocks.11.attn.proj.bias | blocks.11.attn.proj.bias\n",
            "blocks.11.norm2.weight | blocks.11.norm2.weight\n",
            "blocks.11.norm2.bias | blocks.11.norm2.bias\n",
            "blocks.11.mlp.fc1.weight | blocks.11.mlp.fc1.weight\n",
            "blocks.11.mlp.fc1.bias | blocks.11.mlp.fc1.bias\n",
            "blocks.11.mlp.fc2.weight | blocks.11.mlp.fc2.weight\n",
            "blocks.11.mlp.fc2.bias | blocks.11.mlp.fc2.bias\n",
            "norm.weight | norm.weight\n",
            "norm.bias | norm.bias\n",
            "head.weight | head.weight\n",
            "head.bias | head.bias\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tarfile\n",
        "import os\n",
        "from torchvision.datasets.utils import download_url\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torchvision.transforms import ToTensor\n",
        "from PIL import Image\n",
        "from torch.utils.data import random_split, DataLoader\n",
        "from torchvision import transforms"
      ],
      "metadata": {
        "id": "ADrABG7u2kG7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_url = \"http://cs231n.stanford.edu/tiny-imagenet-200.zip\"\n",
        "download_url(dataset_url, '.')"
      ],
      "metadata": {
        "id": "Zwla07jo4-or",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68,
          "referenced_widgets": [
            "1c8e77966b0849b894daecbdf3959d71",
            "b59c7f8a45244e098275cc0b62a947fd",
            "1a064594a1024a73a6fda04a37eaf22b",
            "324958b2dedb4e95b1e9e9cf27017ea3",
            "f9a51471ac3f4437b0e21cf3f81cc5e2",
            "871f263769ad4aec8c58fafccc342129",
            "48950fb206f847d69bf372c390192dbc",
            "a061f9b43bf84e299c3bacc041037fe1",
            "47482b97360b476690de1c28f4153d5c",
            "2c84d3fd1fad41f4b585b4f7f69bbace",
            "06105fa26bab4c31aac2e5f96f06e194"
          ]
        },
        "outputId": "dbf60766-121f-4499-ccba-e2557c9311c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://cs231n.stanford.edu/tiny-imagenet-200.zip to ./tiny-imagenet-200.zip\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1c8e77966b0849b894daecbdf3959d71",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/248100043 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "with zipfile.ZipFile(\"tiny-imagenet-200.zip\",\"r\") as zip_ref:\n",
        "    zip_ref.extractall(\"data\")"
      ],
      "metadata": {
        "id": "xzVF0xZ25A92"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def format_val():\n",
        "\tval_dir = \"data/tiny-imagenet-200/val\"\n",
        "\tprint(\"Formatting: %s\" % val_dir)\n",
        "\tval_annotations = \"%s/val_annotations.txt\" % val_dir\n",
        "\tval_dict = {}\n",
        "\twith open(val_annotations, 'r') as f:\n",
        "\t\tfor line in f:\n",
        "\t\t\tline = line.strip().split()\n",
        "\t\t\tassert(len(line) == 6)\n",
        "\t\t\twnind = line[1]\n",
        "\t\t\timg_name = line[0]\n",
        "\t\t\tboxes = '\\t'.join(line[2:])\n",
        "\t\t\tif wnind not in val_dict:\n",
        "\t\t\t\tval_dict[wnind] = []\n",
        "\t\t\tentries = val_dict[wnind]\n",
        "\t\t\tentries.append((img_name, boxes))\n",
        "\tassert(len(val_dict) == 200)\n",
        "\tfor wnind, entries in val_dict.items():\n",
        "\t\tval_wnind_dir = \"%s/%s\" % (val_dir, wnind)\n",
        "\t\tval_images_dir = \"%s/images\" % val_dir\n",
        "\t\tval_wnind_images_dir = \"%s/images\" % val_wnind_dir\n",
        "\t\tos.mkdir(val_wnind_dir)\n",
        "\t\tos.mkdir(val_wnind_images_dir)\n",
        "\t\twnind_boxes = \"%s/%s_boxes.txt\" % (val_wnind_dir, wnind)\n",
        "\t\tf = open(wnind_boxes, \"w\")\n",
        "\t\tfor img_name, box in entries:\n",
        "\t\t\tsource = \"%s/%s\" % (val_images_dir, img_name)\n",
        "\t\t\tdst = \"%s/%s\" % (val_wnind_images_dir, img_name)\n",
        "\t\t\tos.system(\"cp %s %s\" % (source, dst))\n",
        "\t\t\tf.write(\"%s\\t%s\\n\" % (img_name, box))\n",
        "\t\tf.close()\n",
        "\tos.system(\"rm -rf %s\" % val_images_dir)\n",
        "\tprint(\"Cleaning up: %s\" % val_images_dir)\n",
        "\tprint(\"Formatting val done\")"
      ],
      "metadata": {
        "id": "YHhcouTH5H5g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "format_val()"
      ],
      "metadata": {
        "id": "7npQ_pql6zwg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb865442-4f2e-481f-fbac-4476f08ae5bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Formatting: data/tiny-imagenet-200/val\n",
            "Cleaning up: data/tiny-imagenet-200/val/images\n",
            "Formatting val done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val = (os.listdir('data/tiny-imagenet-200/train'))\n",
        "train = (os.listdir('data/tiny-imagenet-200/val'))\n",
        "print(train)\n",
        "print(val)"
      ],
      "metadata": {
        "id": "gupTtcBc5U9f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5bb079d1-ea01-4435-e8f6-d4b6a1c32408"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['n03388043', 'n03584254', 'n02906734', 'n02823428', 'n04133789', 'n02190166', 'n02099712', 'n02058221', 'n02917067', 'n02085620', 'n02437312', 'n04597913', 'n01945685', 'n02123394', 'n03992509', 'n03937543', 'n09428293', 'n03447447', 'n02927161', 'n03089624', 'n02883205', 'n03424325', 'n03763968', 'n03854065', 'n03444034', 'n04265275', 'n03400231', 'n02123045', 'n02206856', 'n03796401', 'n03599486', 'n04560804', 'n04366367', 'n09256479', 'n07579787', 'n03733131', 'n04285008', 'n04399382', 'n03983396', 'n04311004', 'n03970156', 'n02795169', 'n03649909', 'n03902125', 'n02509815', 'n02364673', 'n02236044', 'n03617480', 'n03250847', 'n03930313', 'n02480495', 'n03179701', 'n01910747', 'n01644900', 'n02395406', 'n02124075', 'n02814860', 'n04465501', 'n02802426', 'n07720875', 'n04008634', 'n02099601', 'n03976657', 'n01774750', 'n02843684', 'n02132136', 'n04487081', 'n02504458', 'n02415577', 'n04507155', 'n04596742', 'n07768694', 'n09246464', 'n04540053', 'n01950731', 'n02268443', 'n03706229', 'n01855672', 'n03404251', 'n02963159', 'n02730930', 'val_annotations.txt', 'n03393912', 'n07753592', 'n04562935', 'n02129165', 'n03838899', 'n03026506', 'n03980874', 'n04371430', 'n01944390', 'n02481823', 'n02423022', 'n02226429', 'n01768244', 'n03042490', 'n03201208', 'n01917289', 'n02056570', 'n04254777', 'n04532106', 'n07711569', 'n07875152', 'n07873807', 'n03085013', 'n04070727', 'n02165456', 'n07614500', 'n04259630', 'n04067472', 'n02988304', 'n04179913', 'n03255030', 'n02486410', 'n03804744', 'n01984695', 'n03100240', 'n02281406', 'n04376876', 'n02999410', 'n03662601', 'n02841315', 'n02950826', 'n07747607', 'n04149813', 'n02666196', 'n03544143', 'n02699494', 'n06596364', 'n01770393', 'n02002724', 'n01443537', 'n03014705', 'n04328186', 'n01742172', 'n02125311', 'n01698640', 'n02814533', 'n09193705', 'n02808440', 'n02279972', 'n01641577', 'n04456115', 'n02113799', 'n02233338', 'n04532670', 'n02977058', 'n01882714', 'n04099969', 'n02231487', 'n02074367', 'n03637318', 'n07920052', 'n04486054', 'n04356056', 'n02410509', 'n09332890', 'n03814639', 'n02791270', 'n02837789', 'n02321529', 'n02892201', 'n03670208', 'n02788148', 'n02403003', 'n02815834', 'n02948072', 'n02793495', 'n03977966', 'n07871810', 'n04118538', 'n07715103', 'n07695742', 'n01784675', 'n07615774', 'n03160309', 'n01629819', 'n04417672', 'n03355925', 'n01774384', 'n02909870', 'n04251144', 'n02094433', 'n04398044', 'n07583066', 'n02769748', 'n04023962', 'n01983481', 'n03126707', 'n03770439', 'n04146614', 'n04074963', 'n02669723', 'n07734744', 'n03837869', 'n07749582', 'n04275548', 'n02106662', 'n12267677', 'n04501370', 'n03891332']\n",
            "['n03388043', 'n03584254', 'n02906734', 'n02823428', 'n04133789', 'n02190166', 'n02099712', 'n02058221', 'n02917067', 'n02085620', 'n02437312', 'n04597913', 'n01945685', 'n02123394', 'n03992509', 'n03937543', 'n09428293', 'n03447447', 'n02927161', 'n03089624', 'n02883205', 'n03424325', 'n03763968', 'n03854065', 'n03444034', 'n04265275', 'n03400231', 'n02123045', 'n02206856', 'n03796401', 'n03599486', 'n04560804', 'n04366367', 'n09256479', 'n07579787', 'n03733131', 'n04285008', 'n04399382', 'n03983396', 'n04311004', 'n03970156', 'n02795169', 'n03649909', 'n03902125', 'n02509815', 'n02364673', 'n02236044', 'n03617480', 'n03250847', 'n03930313', 'n02480495', 'n03179701', 'n01910747', 'n01644900', 'n02395406', 'n02124075', 'n02814860', 'n04465501', 'n02802426', 'n07720875', 'n04008634', 'n02099601', 'n03976657', 'n01774750', 'n02843684', 'n02132136', 'n04487081', 'n02504458', 'n02415577', 'n04507155', 'n04596742', 'n07768694', 'n09246464', 'n04540053', 'n01950731', 'n02268443', 'n03706229', 'n01855672', 'n03404251', 'n02963159', 'n02730930', 'n03393912', 'n07753592', 'n04562935', 'n02129165', 'n03838899', 'n03026506', 'n03980874', 'n04371430', 'n01944390', 'n02481823', 'n02423022', 'n02226429', 'n01768244', 'n03042490', 'n03201208', 'n01917289', 'n02056570', 'n04254777', 'n04532106', 'n07711569', 'n07875152', 'n07873807', 'n03085013', 'n04070727', 'n02165456', 'n07614500', 'n04259630', 'n04067472', 'n02988304', 'n04179913', 'n03255030', 'n02486410', 'n03804744', 'n01984695', 'n03100240', 'n02281406', 'n04376876', 'n02999410', 'n03662601', 'n02841315', 'n02950826', 'n07747607', 'n04149813', 'n02666196', 'n03544143', 'n02699494', 'n06596364', 'n01770393', 'n02002724', 'n01443537', 'n03014705', 'n04328186', 'n01742172', 'n02125311', 'n01698640', 'n02814533', 'n09193705', 'n02808440', 'n02279972', 'n01641577', 'n04456115', 'n02113799', 'n02233338', 'n04532670', 'n02977058', 'n01882714', 'n04099969', 'n02231487', 'n02074367', 'n03637318', 'n07920052', 'n04486054', 'n04356056', 'n02410509', 'n09332890', 'n03814639', 'n02791270', 'n02837789', 'n02321529', 'n02892201', 'n03670208', 'n02788148', 'n02403003', 'n02815834', 'n02948072', 'n02793495', 'n03977966', 'n07871810', 'n04118538', 'n07715103', 'n07695742', 'n01784675', 'n07615774', 'n03160309', 'n01629819', 'n04417672', 'n03355925', 'n01774384', 'n02909870', 'n04251144', 'n02094433', 'n04398044', 'n07583066', 'n02769748', 'n04023962', 'n01983481', 'n03126707', 'n03770439', 'n04146614', 'n04074963', 'n02669723', 'n07734744', 'n03837869', 'n07749582', 'n04275548', 'n02106662', 'n12267677', 'n04501370', 'n03891332']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = 'data/tiny-imagenet-200/'\n",
        "num_workers = {'train' : 2,'val'   : 0,'test'  : 0}\n",
        "data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.RandomRotation(20),\n",
        "        transforms.RandomHorizontalFlip(0.5),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.4802, 0.4481, 0.3975], [0.2302, 0.2265, 0.2262]),\n",
        "    ]),\n",
        "    'val': transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.4802, 0.4481, 0.3975], [0.2302, 0.2265, 0.2262]),\n",
        "    ]),\n",
        "    'test': transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.4802, 0.4481, 0.3975], [0.2302, 0.2265, 0.2262]),\n",
        "    ])\n",
        "}\n",
        "image_datasets = {x: ImageFolder(os.path.join(data_dir, x), data_transforms[x]) \n",
        "                  for x in ['train', 'val','test']}\n",
        "dataloaders = {x: DataLoader(image_datasets[x], batch_size=100, shuffle=True, num_workers=num_workers[x])\n",
        "                  for x in ['train', 'val', 'test']}\n",
        "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val', 'test']}"
      ],
      "metadata": {
        "id": "cXxLnkZr5YfK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_device():\n",
        "  if torch.cuda.is_available():\n",
        "    return torch.device(\"cuda\")\n",
        "  return torch.device(\"cpu\")\n",
        "\n",
        "def to_device(data, device):\n",
        "  \n",
        "  if isinstance(data, (list, tuple)):\n",
        "    return [to_device(x, device) for x in data]\n",
        "  return data.to(device, non_blocking = True)\n",
        "\n",
        "class DataLoaderWrapper():\n",
        "  def __init__(self, dl, device):\n",
        "    self.dl = dl\n",
        "    self.device = device\n",
        "\n",
        "  def __iter__(self):\n",
        "    for b in self.dl:\n",
        "      yield to_device(b, self.device)\n",
        "\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.dl)"
      ],
      "metadata": {
        "id": "348HPOgo5fFK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "def show_example(img, label):\n",
        "    print('Label: ', label, \"(\"+str(label)+\")\")\n",
        "    plt.imshow(img.permute(1, 2, 0))"
      ],
      "metadata": {
        "id": "ECB34hmW-Lel"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "train_dataset = image_datasets['val']\n",
        "show_example(*train_dataset[49])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 305
        },
        "id": "GMU4CUgv-NS9",
        "outputId": "47aa23b9-78ef-4ef7-ec12-3a81ce544ef6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label:  0 (0)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD7CAYAAACscuKmAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO2dfZxVVdXHfwsHAnF41QEBZVQIVEywkUTNNxLNeMSMFB7fenyh0hKNCi0zUuuRzBSzMjLTBw1IzJcIA0JBShEnoQAReQkUkBcBYaIZZ8bZzx/3zN2/vZhzuDPM3AHP+n4+fO46d+97zp5z7+astdfaa4lzDoZhfPRp0dwDMAwjP9hkN4yUYJPdMFKCTXbDSAk22Q0jJdhkN4yUsE+TXUTOF5EVIrJKRG5prEEZhtH4SEP97CJyEIC3AJwLYD2A1wCMdM690XjDMwyjsSjYh88OBLDKObcGAERkCoBhAGInu4hYBI9hNDHOOanr/X1R47sDeIeO10fvGYaxH7IvT/acEJFRAEY19XUMw0hmXyb7BgBH0HGP6L0A59xEABMBU+MNoznZFzX+NQC9ReQoEWkFYASA5xpnWIZhNDYNfrI756pF5GsAZgI4CMAjzrlljTYywzAalQa73hp0MVPjDaPJaYrVeMMwDiBsshtGSrDJbhgpocn97IZh5IGPRa+V8V3syW4YKcEmu2GkBJvshpESzGY3jAORQ9RxYfT6XvxH7MluGCnBJrthpART4w3jQKRPzPs74z9iT3bDSAk22Q0jJZgabxgHIv3UcZfodX38R+zJbhgpwSa7YaQEm+yGkRLMZjeMA5GT1fGA6PXZ+I/Yk90wUoJNdsNICabGGwcG55I8u4mvpTeZdCN5gGqbmuM5jyZ5jWo7hWSuvKBLqxSRfIZqmxa9lsUPwZ7shpESbLIbRkqwyW4YKcGKRBhGc3M2yetI/pzqV05ykWp7IXpdArh/N7BIhIg8IiJbRGQpvddJRGaLyMrotePezmMYRvOSixr/KIDz1Xu3AJjjnOsNYE50bBjGfkxOaryIFAOY7pzrFx2vAHCWc+5dETkcwFznXNx2ej6PqfHNxcfV8VvNMor9lxNJ7q/aHmvka52rjtuQzHWQj1P9+pJ8jmprF73eDrh/NW6tty7OuXcjeRP8BjvDMPZT9jmoxjnnkp7YIjIKe4YHGIaRZxo62TeLyOGkxm+J6+icmwhgImBqfLNyIKrtHFm2oIHnuJ/km3L8zFPq+G8kn6baOpD8PskXq35/IFnPOl7e5r9Zq+ocyadn3OLo9d+IpaFq/HMArorkq5C418YwjP2BXFxvkwG8AqCPiKwXkWsA3A3gXBFZCeAz0bFhGPsxe1XjnXMjY5oGN/JYDMNoQiyCzjgwOIjkG1XbfTmeI2nnGXOEOn6H5JsTrs275Yapfk+QfJBqKySZ7XKdoIKPX1FtL0WvmwBX2biuN8MwDjBsshtGSjA13mgcklTfjxId1DGvenEFVa2qsxttkWrbQfIQknUiihEJ41oRvU4F3GZT4w0j1dhkN4yUYJPdMFKCJZw0GoePqo2umamOHyd5KeJh91pL1Xa9Fzs/4OVtd6l+7Uhuo9o2Rq8Jj297shtGSrDJbhgpwdT4tHIZyYWqjfOb3RHzGSCMCmsouUa1NSd3klyh2rh08s9Ivl3148/NUm20g23bWnr/e6ofJ7MYrdpq88g/iVjsyW4YKcEmu2GkhPyq8a3h1bY38nrlpkVHS31I8idV298TzsOq32KSX1P93kU8nKyBI7DWqX5jSf6FarsDdaOTKbyQ0MYqPidy+IPql6vqfijJ16u2uPFqOM/cP1Tb6V5sOz9s2s056bRqfXjd59gjuQR/F+WqjUtI8fd0uurXneThqm1S9KrNDMKe7IaREmyyG0ZKsMluGCkhvzZ7BQ4sW/2z6vh5kilRQdclYbdNnGRA2+jsaroubGpF5YAr2T2j7TNG2+JTSGaXmi4XxGsCuvzvb0nmJIqLVT9eO5im2r5C8kPYd3hHWVLGw0PVMX+O7fSPqX6Uk313nXvGIr4SHrai9Y5KzulUrT7H+ZV7qrZ7Sb6cZH2/aV2ka6ewaVPt+RNy3NuT3TBSgk12w0gJlryiHrR/3cs7WbXW1Tb/RHJ9osI4h1kVyceofux60gkObiWZTQZ9Dk6goDdmsHr+KsnnqX7vIx52c7EJMU714/uoXYoXkvwc4uEIt5dV2/PIDf47xyWc439V20aS2SjurvrtIlm7Ctk0IFNuj8hGbtuOurkacG9a8grDSDU22Q0jJdhkN4yUsH/a7Ier46Tw0H3ljwltj6tjDmtkt5zexZRkz49FPJyQgHOjv6T6cchp0r3iXWqDVD92ryWF8OaKdmV90AjnbAhq19d59F38Y4OXN/VQn2OX3XuqjZJMtlKuzsr2dEBlsdsqt9nuOXSg2lp918vHkLWta6A/8yZ9pm/YVrkwEq4C3PIG2uwicoSIvCgib4jIMhEZHb3fSURmi8jK6LXj3s5lGEbzkYsaXw1gjHPuOGRqTN4gIscBuAXAHOdcbwBzomPDMPZT6q3Gi8izAB6M/p1FZZvnOue05qE/e0C73mJJ2vX2G9XGKpy+W7xjiV1Iq1U/dre1Vm1Xx5xfRVwFrrcHVBu72H6G5oNdkQmliAOTKsnVxqaGNtG+mOugFNeSTCr+Ccq0W0Lf572nhmF+Yy73dsNtjx+Vle+a+K/wJJTjrr36znbWqvjDAbe0EVxvIlKMjLfvVQBdnHO1FuImAF3qcy7DMPJLzrHxInIIMmXqb3LO7RLx/3k451zcU1tERiGMDDYMoxnI6ckuIi2RmehPOOdq14I3R+o7ole9XR8A4Jyb6Jwrcc6VNMaADcNoGHu12SXzCH8MwHbn3E30/j0Atjnn7haRWwB0cs59ey/naj6b/VdebEHusBqdf5tcY+f9+uNB00x5yx9Q9pXeT4WnWEnn6Kxcbf3Idp73TNg28iIvT/4GNehdUhw2OUm1sb3NGW7eVP04hFXnOx9HMrsOP1T9OMxT79BagPpztDqOCzU+UR3zbrZT4k8/lMocT39ENfL4tbv0/PhzBvD96Bfba4/7Heyc47WVX6vPfZVklU0nS4LNnosafxqAKwAsEZHaW/IdAHcD+L2IXIPM0sQlOZzLMIxmYq+T3Tn3VwBxO3wHN+5wDMNoKg6MvPGcsJDVnJNVvy+TrJIM8E6xGlZH71T9KEHAzE+/FbZdSjK5vEqUXrn9Lq9X9lE7yngzW5CwEcBTHBXFauDGsF9QgkiZAkG0He+a0ufg8w9Rbaxm8i9Eh001RlIKJtcdgjpZJJeL3qDayFwJtOeLVD/6LlqoZB41D9KBVq15LBxdp77boNzyyLCpDT1Ku1H55rU3hv2mb705Kw+deF/QNnZUJrPpY62XIw6LjTeMlGCT3TBSQn43wrQTh1oHnDYg2BN/r2rLdWWXA5PURoHO833jtutop4NOdpCUI4+i4QZe7W2LhW+qZOh3k3xF2FRMqxw7VAKCnbxyzxtXxqtxsAqqN8mwA5RX9HUUHq/O6xx0nACD897rDTMckZa08YXNn6lh05n085t3pPocVYY9lvot1ytIvAKvfyvnkszfha7Gyiahvlf8e5yi2uKiFHVue/o9nvlAGEE3b5HeeZPhvAHh8Uwax4QxYUGC0XdHX86DgFtvySsMI9XYZDeMlGCT3TBSQl5dbwf3BvpFttLCVqqRopv0DqeRzkeyTeYotqQSwipibFtrsovYJaVsapR2pQFvCtvO8EZqNfl0hvcN44mmvfB7+kx4irULvTx0YNg2ndxEbelzu3upMXK+eb2+wbutOOnh11S/v5Gs7wHv4tPJKBmOPkyy2afGN81jm1dfi3a9BXb6VaofJ/jsoNo4fzv/2nWiTi6BrNdxyIZvpe5VJR+sJVnn86frzTsytNGffNu71AZRZtAe3w+/tEk/8Lb+kSrj5D9vybh/R0xT7mLCnuyGkRJsshtGSsir663gRHEdZmTkbVp15Cgu7Qo6LeaEr6rjTyVcnFV+Uu3OnHJx0G3el7wbbcKjfwva/kD64opybydsek253jjy6TY1Dgqh05Fa3bp5eT1HYI0I+wXq7rj48yeqprzxQ5duYtgcOlW18ef0JhlWtSkqbI+NO5zDTW9wIbNmLJkn41W5rcAVqTf8sKuMo+t0OSxymw1W0XUcfLhcRSxedJFPNvHMEJ9s4opZZwX9Jv18rj/4U9CEY2mMBaT+33xC2I8tkurNYdtlUTaJM0uARaXmejOMVGOT3TBSgk12w0gJebXZ25aI61uakV9XVkV78nLt1HWytD3YEM4mmcIyhz4a2uzTxdvfk93coO0VrMjKRWT0vaiyEcx307Ny5ZRwYeHekX6b3Zg+3wvaevvTYyW7B3Wuee3WiWvjJAzKTgzCOV9Tbdx3Isl6TYRz7v+XauPlDnZ5va36UeKGIMy4ruvVdW4gHL8u59yfZF6n0LvS+B5/V7WxsfwL1UZrMndR9snbdj0cdGtR7uWarkETWtGuug9uuCcrXz3pW0G/5+n+vLtMP6drAAAlJUCp2eyGkW5sshtGSshrBN1/tgOvT44OQu0ZO0m1GejaBm0LC3f7g6T84Um8SDKVAj5e3YLp5P65Z1GYFPfWAeOycj9KDHfbjZ8P+o18wEfQHTkyTHK3mHTkrioX2UYKijqBIq6W6G9pAsljVBsPmXfA6bzxrI7qKDxW3XmMWn0+Lcc2Lsk0XfXj3X067xHtWOtMrrcLVLdJ5LIMyloBYU4+UqU5SQkAjCW1frzafXcmmR7blSv1Uvoh9wENRO2cu4x2sG15PWybeZKXR1d41f0eNcaTKZLyi1ITtN0fBeFVvYNY7MluGCnBJrthpIS8rsZ3Lunqziu9EgAw+bp7wkaOaPpR/DnOdD7h2DxROgtv4NAr+Lwa/2kvDh8T/n83rb1Xj1qpxNg7xnv1fAEtAU8NQtXCVG3jh4TlQnvM8rrppbTpAQDuFZ/U7Tz6WmZ+GiF8r1SCgyCZBavu/VU/3nik0x7z6jNHaqncacGqvU6iwbnZ2GT7H9WP1W69Qn6XFyfc4E8yeoSKWORU29pzwcesWuuqtvxVqMjG60l9nqE+tpVMr4c6+Q1RFyqbpIxCGLur3OCfOeeOrFxO5uYQVVbs++xBUVUaZkabjb4O4C1nq/GGkWpsshtGSrDJbhgpIa82e4+SE92NpRmrZ+zPVb3iB8i9pneKXUkyuy1OQu5Q+afOZPNuu0v100kVCee8W+Q1MlJnqCyHT1E2w3sC3w9wB2WBfFnl/v7zqPuz8vlyk2/QJaFpzG2VDbybd2yxe+26sF9gl+oqfWyz8+deUf3Y1l+k2u4g+S8ka5t6OMmqzFUPWiMoouyWr49QmTL4p6QTfZC77VRyS76scrJjlxdPejRs4mjPeS70GZ/5iF8/OJXu6fO9wsIF7Y/06zHjVZnTqRRIyT9v7aWkAMugUjQAtIteSwCUNtRmF5HWIrJQRP4hIstE5AfR+0eJyKsiskpEpoqIzj1jGMZ+RC5q/AcAznHOnYjMmu75InIKMgmO73PO9UJmx/I1TTdMwzD2lXqp8SJyMIC/IlNP8k8AujrnqkVkEIBxzrnzEj9/+MEOV2fyyZ3ww1B/XrbW+11qKsLQsiv6+swWkyTcHBBwHMlaNeVEAOyGCjVpDP0/L0+/Mmy7kUyBy0n9fLFTeA85f8QylU1hHSln2qM2/hH/txWTer5D/S07OX+adnmxGsvJJbTrjf9rXqnaepO8nmRd7XUVySrRQuB//ATJf1b9eOOOSihx0kU+odzr173vGwrDfpws5NTisOll/jK4lkB52C/IG68q+w6l5BIrVHKMa/v6vHBjv0S55R4L+7WnVPEdVZr4tSSfSfLcS8N+Sbn8atknNR4AROSgqILrFgCzkfFYvu+cq7Xu1iNMb2gYxn5GTpPdOfehc64/gB4ABmKPeivxiMgoESkVkVL8p3rvHzAMo0mol+vNOfc+MltKBgHoICK1WzR6YM/6mbWfmeicK3HOleDgA6NorGF8FNmrzS4ihwGocs69LyJtkMlMPh6ZlIJPOeemiMhDAP7pnNNb+wNa9T/WdXkhY8ys/1O4PWniFb/Mys8pp8P0Ep8ZYXipz2RYqLI4/FbmZuUJ7oigrZCM+Kv7k+tGl//NkYlPe3nFReE9ZJMyDIgFfkwZGrTN3p+Si3+r3McM79R1ySjpYVtVB24374Lj8FOdcHIcyTqRJK8DsC2rXGPBH6dzvvOOLR6T+v/+BIo01nrfRnKHXdzO/wYe2/DHoF8Nn38iQtiNy3b6w6of161TawJjtnr5Xm0Nc9m2uku21QteLtGp7d/lA52cMwqNLikFSnfVbbPn8qg9HMBjInIQMprA751z00XkDQBTROQuZLysv0k6iWEYzcteJ7tz7p/Yc7sFnHNrkLHfDcM4AMhrBF1hd3Gf/GpGnqci11rRDqr+KonBMaSzrKPEai8/EyY8/9tFPgtDTxWqtfhuX4v5VipJvBUhnO58vmpjTZiD91QugsAI+ZwLd/e9Sf6rh5W+WIouWXkeL4G8/FDQD6eSDrdQ2SG8E40tJfXfdTGp4F9XCTDGcJ56vo1ajWe1eIVq4+pEtJzb/uQwMUm3Tt6J003dj9VVPpxxLZkrV9wWJnGbD5/AcO1khHDSdz79l5EzxSSvTejHKrjWsp9M2pHJpsAPSe4WdvtvyvP3Oz1t52ReSq4HSlfYrjfDSDU22Q0jJeRVjZc+4rKbM1SusM6kSm5T5Yha0LLk7Ad8KuZzVCaESlqmXrAhrIB5Ma0qbwuWNXOHKxVxmrKNqt+2HM83RlUjPftRH0G3kcLJXlHhgGsoKm9+eZgjumYR5SbjFMt6dYZW6tuOC5t28zIw/3ElYb/OxV6u0BFpu/zydmE7H07XsU0Ye1VAA1OBawAqslKfILwuvB/cdtvLs8NTsMOGf3M68jCpCi3RWR1zZasPaSotUq6W/n/1sv69BHfkfpLVhplFlBp8wBx1kigxR8nngdIlpsYbRqqxyW4YKcEmu2GkhLza7F1KjnMjSp8AADxQFRqAw1t6W3OaymuBt+o+n/uXiuMpvjorfkpCs2Vh/YZaJ8UkryVZ52Fk70+xamNvyhrVxt6q2Z/1ctFPLwv6zerrt3LNUBbgzF1kHHJIms4bT/RQx/yxTRy9p3bOndDlk1m5jdqyVk4WeJvAGg8XD6rpagWqrYB8exdQmN99m8P6TP26eKt3S7AVDyir8r+r9WynfwaNwmTamTaSdqXpnCjfOcTLK1Ttg76cDJV/IMrVeQMlYv3fo8O2dlFOlJLxQOnbZrMbRqqxyW4YKSGvavwRJb3czaU/AQCMmRCWTGI1s5Vy41TWI9qpOdDuGKoutcfmDlZodRoOzlrGwV4Xqn6f/pXP0tFt1A+DtmsphG4yEnTwYGR6y4U3BdqX+2sVtQlV9Y5BpoxQBWc1viX9NYXKnigk9X97YMgARbS7hlX6BbtCd+P6V7y7bcJ5vwvaFtD9+DQl3rtMlWptL/u+i+VGipJ7UUXJsdGqfxNDSD6e1P0BqizXtkle7qwTeES3sWQ6UPqeqfGGkWpsshtGSrDJbhgpIa82e7cScV8uzch3hB4S1HCiBVXuNthu9lc0CLarcw1nbQqGkqw2NQWWMwf7nqb68e6qe0KvHFaPOT0rvzLA+24WqAyL64OQ09AWb0Xb1IaQvb2na8x/TtuhZUE2C2+/F6q/uiiw2SuCNl7f6BNkygj/lvkUJn0zwiyhU8kROgzD/LUmh7vvrubQ1ISy4JPuDI+fGufl2WSna+8x/6QfUm6zPnT7B/AH1U7FNzk1jMr4+KOoRtyfAGyzWm+GkW5sshtGSsirGl9QIu6QSI3fqdR4znHeWZXM/RIFif3si16ubNTR5R8dZcVpwdlVs0z1S4oGLCb5xT96Z97WoWFK//s2+yQa27uESucF8Dd5Bh7Jyi2VGl9IIV4dlV5ZTn23kpJfpc7RJlu4COim3HKrKYHHtXR+fT94g5muUMVn5FT23RZeEPS7bNzzWXnBjHA7Yndy2T27KKyFMmyANwemDPElzJ5Qm++Op4jIO34ati1jDymZs9WqpNZiqlB+jDLfPpsJTEUVgBpT4w0j3dhkN4yUkNdE7h/WADtrF1J1eabrvdhvcdh07xfxkaRCHb9N8hdIVpWEgo036lYFqmrxCq9mF38utI1ubedX3B/aFa5uL2jHJQC8IlwFTfwqeyG1lQWr52HO6YI9clB7jiHVnVPcbVf92Iuhf1as4vP9PWNgGHn4zowZWXmzKkk7jdN/D/h40HZWid+lNZdU8C2qEvG3KNnE+GPRILjg7Q6l4udi0tqT3TBSgk12w0gJNtkNIyXkt/haOYAlkayz7hE6pzyT6Cp8yRsycuZJ8f32E55VxztJHpfwObaA9V62U/igiKLOVuwK+hX19TZ7f5Xp8TmS+5BFvFVZ7QXB3rzwJC3hvT/dcHBMrzB2T1eE/jrJ7Jb8kuq3gGRdUosTQnL+zctVeBrfndWqrRt80vqudx0VXsCntsdZlBDy+OFht4oHfHL4jWradcfjdOSzaqj8K0EFbrwRtmULHkxHLDk/2aOyzYtEZHp0fJSIvCoiq0Rkqoi0yvVchmHkn/qo8aMBLKfj8QDuc871QuY/0Gvq/JRhGPsFOanxItIDwOeQKU7zDRERZOp0/nfU5TFkNM9f1nmCWjYDqI0e0lVFqbzPpFkXB02X49e5DBM4wyuFYz4WNt2bY17wfLJk713q5JmEtiDRRR8fubbqmNDR9zAprssoig0ASoMj74Y7TCVF607HrRHqnHy1eOdaqGbrirccKccRhXqf1GEkazOBN+hok4fhUl/Hq7avU/L5sbeFKUcWDPLlvZ4b7Is+bVcmT4tzvN495oWwfNVPuD4WFSBr3yF0upaQmTDnCTXI2ptcZ+xcNIb4poD7AXwbQG32vs4A3nfO1d7L9dhjH45hGPsTe53sIjIUwBbn3N/31jfm86NEpFRESnOtumEYRuOTixp/GoALReQCAK0BtAMwAUAHESmInu49wPoe4ZybCGAiAEgnyd+uG8MwAnKpz34rgFsBQETOAvBN59xlIvIkMhF8U5AxNLQnaQ8+efQnUTolYxEuUkGPBWRDFqls65xekMMhdYKAIrLY1l2vGu/b2+g+GjzxHa+snTLQJ2t4m9xfQHgfdbXlTSS3pnDZQlVSme10nf+wTdDPo+331oinMEZOIukHndTG6wA6BLmKdr3djjODtgWDfT77xeTcu3XXj4J+8154OiufgYuCtkPpPm67G7E89LhfEzj7trBU9/ranXMzEcu+BNWMRWaxbhUyNvxv9tLfMIxmpF5BNc65uQDmRvIaAAMbf0iGYTQFec5B199dU5rZGlSufG/9ScE7XinoOwKXj1d59P9Uy/CfrHy9hDnGdjqvRrVTahTz+He/n5Wv+NEdsf0aCo/qlRPDtk+EmlksNe7PWfk+Kt8MACvIjVMOX6e6VCnM5aTWq0rMQVRbOTZn5e7oEvTjfW5azd5zh1zd7Fmm2cNlsZPUeHav6X78G6lI6JfkSvoxyR1V27dwXVa+lH63l+HqoF85mUPLguJhwBg3KCu33+idkQ92vyLodzm8701E1V2oLUM1C3DbLXmFYaQam+yGkRLyqsZLiTgVnpWF1dtLEOq3l+K3WbmENinorNI/pgqeLw/pHbRNn/WvrLyDtlwcptaHC+kcp8nXkE843fV77ldZ+T8IE0/sIFX9Z0phnk/HZaSq61VwVpEHqTbOdMybTIpUP1bj9ao6X4837mj1mY93qLZc1XhGq9lsJvD5tdrOHp97VBtv0NFFosINNB5tYvK9OvSCUMv+xQw/B79K7/9a7WppQz6UK15S0ekTo9fnAbfN1HjDSDU22Q0jJdhkN4yUkF+bvZc4/CQ66Kka2chRBlXvTt52vhg/y8qqsjPYc3W7ansF/5eVe1LmjCoV5XvtVx/011Lldy6nXUeXkHzz4leDfv3oDzhbJWLcQlFW1SqDx8nkWmGrN3SuhWsTa9T52aXGLTqpA7dp9xfvIuP059pm537apmY3F9vz+lp8rBNJxrn2dKkpJslm58SUOvf8OQnnTIJLiXFCywG6I/GaOh64gbK1vPBzL18xOOjngiQXIbIkMtNHAG6Z2eyGkWpsshtGSshvDrpN8Ekqxqo21qPCXApYSSot5bgIXFUAcDbJ61RbJ3KMXE6urEXqFnQkn9EFo/4rbCNF8MHFc7PyChU1/EuKkJqvjI2vUd+3EfJjeJOK1VadTOF29MrK2l31Ism82UWrtwUJbf1IZk9pkttMq+dxCSuSNszoqLs2MXIZGgafQ//NnHe9PvnV2FzkVBOnq35sevxFta3v7pPM9xhOd6j1TUG/qytGZOVHgnrA8LbBbsRiT3bDSAk22Q0jJdhkN4yUkF/X2yfE1SYlb1sc5t9uTVZNhUo9eDR8Xa6t8MkCdF2vU8jmHaSK95aQi+2JJQ9k5dVLQ2vzspG+7btBIkDge5hM5/Ofe0HtouPEhtpG5VDMbqqNbWW203WEMbvAzlZtbPcmJXPkNQ292U6PqxZt5+rjONh+12G1XFJZu1L5/HyOJJtdn5+/3eqEfg3Ng857N9eQfHkDz9dQ5OeRt2084N4215thpBqb7IaREvLremvVByh+BACwWyWv2F1+rz94M8wC1nOAj60aQu9vWRs6a44p9n/OEy7Mrr5RfMjexhO8C+PcE24I+5GsI9c+S8WSWdXV6udTJGuVmFXQC1Ublzu6hORhqh+f82TEsyqhjd1+r6g2Pj/f7yTXm4ZVZv6RaRddkguwE+qP/kHHufYWqH5sNmmXbhJbST4ythcorQrQdsTkoG3nFP+7Ul7nAGnttXNXoczvWpvtw/jP25PdMFKCTXbDSAn5XY0/tsjhsS8AALr20uvDnk0zw6zUvUd6tXvlS5RmQOtsx/j17M5ddHoCr9B1o7XpTmrXTRUppyWqjffucPplXVaITQEdFdYpoY1VfFaDtXpbFNMPiFeZkzaIJOWL479Tp+5mdb9CtbFpw9fWm2l4vFp93oa6qY+aHYfOuNEfDPkAAA5FSURBVDyD5An1OI/s8mdy7c6L7XcILsjKu5XhN4biHn+C3BCVACP7B5QArtRW4w0j1dhkN4yUYJPdMFJCfl1vu/8DvLYIALCpQlmbhRTTVBbGSBWyhVnkrb5WhaHzp2cXH/G2entY2Lemyv+pbbr4z1WrW9CGLNhqZbPzqNjqqs9N1G46JikpQxxJrqaChH5JbbnCUW1JawctY94HkvPGcxt/mzqCjtcOco2E09b1wyRPUW0jkEC5z6vPfjOpCmPojm3p01ksnxCWhvrJ6KQL1M3yGWE552N3RaWeE1xvudZnX4vMPf4QQLVzrkREOgGYCqAYwFoAlzjn9I5LwzD2E+qjxp/tnOvvnKstIHILgDnOud4A5kTHhmHsp+yLGj8MwFmR/BgyNeB0SoqQtm3R4uTMRpYje4axZRXl3nmzqSp0Bq3eQErcSz66rrIgVIpX9qTtHf3CTSwoI/W8yH+ujYRpFlqT0qlvDhsNYYmk3ImLLKsPSefIVT1P2hQSl3hCv590LVbBk6qxJqndXHe2X2wvLgi2J+y+4w0/OtrtSZKvROiOHnmvfyb+bUzYNrTLlVmZYzEntgzzxV1H8mdG65jI+tMXV4ZvTIvU+ATdOtcnuwMwS0T+LiKjove6OOfejeRNgCoEZhjGfkWuD5fTnXMbRKQIwGwRCcLGnXNOROqMzon+c8j8B9E117oehmE0Njk92Z1zG6LXLQCeRqZU82YRORwAole9vbz2sxOdcyXOuRJ0SFp7NQyjKdnrk11E2gJo4Zwri+QhAO4A8ByAqwDcHb0+G3+WiLJ/o+aFlwAAa3W4bAFZkavD/zeqy6iiVgENuZ3SFIrIVbZDOWh2+fOXF/pddFVdQou7deB6U0OMkXU/HlV9XG18zqRkjkm10+LQ4axxdrkeR0P7xSWsaGiSiCS7PAk2YTkZ552qH+/B7Kmv9k0vDhnzZNC0Al/Myj0KaVdaWXwY+kMqQWmjUBvL/bH4Lrmo8V0APC0itf1/55z7s4i8BuD3InINMolPLkk4h2EYzcxeJ7tzbg2gyqpm3t8GYPCenzAMY38kvxF0SbBuWhYqv1W7SAktp7aWoRLbotorxjXrwrJO2OLV+p10uh0twxQJFR39+d+WsK2AIurKghzvodoX53bKfM6TlCedr6xdY0mRS3FuOX2tJPMi1yg85NjWGLBSrBeHktxAvUi+mORK1Y/NnO+pNirOhN3q6oGx+AVfeFyf/ysk/xWbg7a3GsORNTjS3wv1lT0WG28YKcEmu2GkBJvshpES8mqzS9tCtDw5U9StqChMrbdjh3eH7Va72VBBw9y4pW4ZQA07fPQ5yqkg8Pbtdb+vKOsUpmzcStZsOVm9O9TuuCR3VZLNHhdWqm32iph+QLy9rd18bLNr116c3a9/LEm73hobXhWpj4XLiR753j+k+o1238/K4+QHQdtg50ty/0W5zR4g2T3676wsE4cE/dyoWVn5uKYINl31Qeb1g/gu9mQ3jJRgk90wUkJe1Xi3cxcqZ2XUmfXHqAi6KlKyNiY4lzhKrihMX9hjgE/9uH5dmLwi0DNZLg8V0Koqr+CWKSdXeSe/q46TXmxU2eELSOnUKjKr7klRbUnRaR1jZCA+oi7JXad/BGw2tIyR90bc7dZxZQ2NjMsVLp3FyTN14stHSHXfqtq+khDxNvp4/xfcuMz/day2a95Qx2xqHIwGMi563RjfxZ7shpESbLIbRkrIc/mn1sCRUfbxIlXcp4wU3vJwSbFyA0XDLaUV8jbhavl63jBTqArpbKFV9wK/ir9DregXFPh+FXozDSXVqKJovZ7dw9X48oTCRXG54YH4/O1JSTSSSjKxmZC0mUaT68p60o+H/5bqmPeB0DRIUuk5LixpM81mdczqOue9b+iKvsaR6s6/xt6i/po7b/efuS1c7W+w6s7U/gQTbC17shtGSrDJbhgpwSa7YaSE/NrsNR9mc8If2/9zQdPbb3vHyG6dGJxdcd28fdxq8KCgW+WkSb5t2PCwrZDsaNpVt3uVKszc0lu3Bd1CA6i8wPs1Vq/26wj9uod/y2wszMrXKrcNOwR1tbs415u+HXFJLoDQvcS2sk6wyOfQ3hq2c9kNp38svNqhx8H2Mbuy9LWKkRu5uv20Ld4YsWq52tS8wy6phuIv1fFX6zsg7Fmrruv4pwEA78355p6dI+zJbhgpwSa7YaSE/KrxlR8C6zKurTJV4olzUqC1+j+IGyu8XLlLK7h0qaVLwzdWkwKtN8kwpMbvUP0KC+vOLlelFW1H41UeGHaBadU0zvWWa4kkfcxqfFKOOH2OuLLPSf1yzUHXUJo60i6faLX98yQ/neM5zsc3guPl+CkAYHiQaiPEnuyGkRJsshtGSrDJbhgpIb82e0UFsDrjHNq4JbRzazjUtZ2yDmknWmC/71CJJ6pqqE2lJSyjEFzOPV+uAkfpnDWtw3FsbeOPK2m33A6E49hR5h1M1SpqNxhufFOAtnmTar3F5azX5yhLaItb0dDvx9n2QGhjW2mQZNhOl0XeaekGrNizcy3D7gsO+z6bsdmT1kfsyW4YKcEmu2GkhPyq8R98CKx+HwBQo9Xs7aRYFqrUAmWkuLLuu12p4C3p/y6lgqMj1cXhHXct1R4vNi+2hGPc3YlSRZRzkotQjd9OOe6qlBqftKOM/5qkfg3J99bQ83FyjPqo8Q3pZ4SquywS1UZReTr7Rg7k9GQXkQ4iMk1E3hSR5SIySEQ6ichsEVkZveqkKYZh7EfkqsZPAPBn51xfZEpBLQdwC4A5zrneAOZEx4Zh7KfkUsW1PYAzAHwJAJxzlQAqRWQYgLOibo8BmAtgbOLJquF3T2xXUWfbSWXupHRfTvfMK/XVKotbG4rbaqP+tJYxa9+Jd0B9hhJWsFytFOHKLX7jzo5ifU7eIBGqaXw1vjta9eW2pCQUuaLPz+p60qabXKPaGlq5Ne2sHLApOJZpdMcHHVXv8+XyZD8KmY1LvxWRRSLycFS6uYtz7t2ozyY0zgYjwzCaiFwmewGAkwD80jk3AMBuKJXdZfbz1bmnT0RGiUipiJTW3cMwjHyQy2RfD2C9c9myGNOQmfybReRwAIhedXFNAIBzbqJzrsQ5V/KR2s1gGAcYudRn3yQi74hIH+fcCmRqsr8R/bsKwN3R67P1unKVsjbX+ZzsKNKlm0jWiSqZQk5aqc5fTtF1BWT1tlS3gJcLOoXXatXRj6uSovAKtfOKogG3qHSFYdmlMC1CWYysv6SKGFkTlwxDt2nYq8Pjtf+rGwedFDPO/u2lWyZ48dT5a+p93Vz97F8H8ISItAKwBsD/IKMV/F5ErgGwDsAl9b66YRh5I6fJ7pxbDKCkjqbBjTscwzCaivxG0AlfUbm1OFqtp1LVuYorl3zapTbCcHIJVRoKFZTHjncLtFGOJz7uGZZ16kRq/aYKn4Outd5+QCbEjj0KL/kx6pvPx0nRdElfWlzlVu1eS1LjE/buGI1Ag91W8z+eFb/dgI9bbLxhpASb7IaREmyyG0ZKyK/N3gLx257K45NHBnTknWe7wjZyo7UYMCBoqqmOCSzVO+zI0G2lXG9VHIJLYkttAVMobfkeAa18y+Ot4yT3Gu840kHAcXnek0pH16cUs9H4sCuO7fmZeDnseNdbWXHYbfW/jj3ZDSMl2GQ3jJQgSWVqGv1iIluRCcA5FMB7ebtw3ewPYwBsHBobR0h9x9HTOXdYXQ15nezZi4qUOufqCtJJ1RhsHDaOfI7D1HjDSAk22Q0jJTTXZJ/YTNdl9ocxADYOjY0jpNHG0Sw2u2EY+cfUeMNICXmd7CJyvoisEJFVIpK3bLQi8oiIbBGRpfRe3lNhi8gRIvKiiLwhIstEZHRzjEVEWovIQhH5RzSOH0TvHyUir0bfz9Qof0GTIyIHRfkNpzfXOERkrYgsEZHFIlIavdccv5EmS9uet8kuIgcB+DmAzwI4DsBIETkuT5d/FMD56r3mSIVdDWCMc+44AKcAuCG6B/keywcAznHOnQigP4DzReQUAOMB3Oec6wVgB4BrmngctYxGJj15Lc01jrOdc/3J1dUcv5GmS9vunMvLPwCDAMyk41sB3JrH6xcDWErHKwAcHsmHA1iRr7HQGJ4FcG5zjgXAwQBeB/ApZII3Cur6vprw+j2iH/A5AKYjk/WgOcaxFsCh6r28fi8A2gP4F6K1tMYeRz7V+O4A3qHj9dF7zUWzpsIWkWIAAwC82hxjiVTnxcgkCp0NYDWA951ztbt48vX93I9MLobaJIGdm2kcDsAsEfm7iIyK3sv399KkadttgQ7JqbCbAhE5BMBTAG5yzgVb9/I1Fufch865/sg8WQcC6NvU19SIyFAAW5xzf8/3tevgdOfcSciYmTeIyBncmKfvZZ/Stu+NfE72DQCOoOMe0XvNRU6psBsbEWmJzER/wjn3h+YcCwA4594H8CIy6nIHEandGZuP7+c0ABeKyFoAU5BR5Sc0wzjgnNsQvW5BpmT6QOT/e9mntO17I5+T/TUAvaOV1lYARgB4Lo/X1zyHTApsoCGpsBuAiAiA3wBY7pz7aXONRUQOE5EOkdwGmXWD5chM+uH5Godz7lbnXA/nXDEyv4cXnHOX5XscItJWRAprZQBDACxFnr8X59wmAO+ISJ/ordq07Y0zjqZe+FALDRcAeAsZ+/C7ebzuZADvIpOzYT0yq7udkVkYWgngLwA65WEcpyOjgv0TwOLo3wX5HguATwBYFI1jKYDbo/ePBrAQwCoATwL4WB6/o7MATG+OcUTX+0f0b1ntb7OZfiP9AZRG380zyOQqaZRxWASdYaQEW6AzjJRgk90wUoJNdsNICTbZDSMl2GQ3jJRgk90wUoJNdsNICTbZDSMl/D/Hp4Rx4xGptgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoaderWrapper(dataloaders['train'],get_device())\n",
        "val_loader = DataLoaderWrapper(dataloaders['val'],get_device())"
      ],
      "metadata": {
        "id": "-fYLkS0N59JP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for images, labels in val_loader:\n",
        "    print(\"device\", images.device)\n",
        "    print('images.shape:', images.shape)\n",
        "    print('images.shape:', labels.shape)\n",
        "    print(labels)\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pu51gogm--1W",
        "outputId": "ea3f143c-6dbd-4a0e-9678-21106b211071"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "device cuda:0\n",
            "images.shape: torch.Size([100, 3, 64, 64])\n",
            "images.shape: torch.Size([100])\n",
            "tensor([162,  45,  37,  96,   2, 101,  14, 127,  10,  37, 197, 150,   3, 196,\n",
            "         62, 189,  19, 190, 188, 174,  84, 190, 175, 147,  53,  18, 155, 142,\n",
            "         28,   7,  26,  39,  10,  90,  71, 175,   5,  10, 138,  46,  90,  12,\n",
            "        112,  33,  30,  41, 164,  96,  80, 194,  75, 172,  19, 150,  21,  62,\n",
            "         64,  41,  86, 106, 134,   0, 163,  19, 190, 198, 157, 182, 150, 130,\n",
            "          9, 108, 100,  22,  19,  16,  59, 100, 167,  68, 104, 182, 160, 110,\n",
            "         39,  24, 179,  48, 123, 143,  77, 123, 186,  49,  84,   5,   8, 138,\n",
            "        190, 152], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy(outputs, labels):\n",
        "  _, preds = torch.max(outputs, dim=1)\n",
        "  return torch.tensor(torch.sum(preds == labels).item() / len(preds))"
      ],
      "metadata": {
        "id": "U83lg8aW6P5U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "di0mHFhEBXQy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def evaluate(val_loader, model):\n",
        "  val_losses = []\n",
        "  acc_losses = []\n",
        "  model.eval()\n",
        "  for images, labels in val_loader:\n",
        "     \n",
        "      output = model(images)\n",
        "      loss = F.cross_entropy(output, labels)\n",
        "      acc = accuracy(output, labels)\n",
        "\n",
        "      val_losses.append(loss)\n",
        "      acc_losses.append(acc)\n",
        "      \n",
        "      \n",
        "\n",
        "  epoch_loss = torch.stack(val_losses).mean()\n",
        "  epoch_acc = torch.stack(acc_losses).mean()  \n",
        "  \n",
        "  return epoch_loss, epoch_acc"
      ],
      "metadata": {
        "id": "klhNNn3p6f7b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "custom_config = {\n",
        "        \"img_size\": 64,\n",
        "        \"in_chans\": 3,\n",
        "        \"patch_size\": 16,\n",
        "        \"embed_dim\": 768,\n",
        "        \"depth\": 12,\n",
        "        \"n_heads\": 12,\n",
        "        \"qkv_bias\": True,\n",
        "        \"mlp_ratio\": 4,\n",
        "}\n",
        "\n",
        "model = VisionTransformer(**custom_config).to(get_device())"
      ],
      "metadata": {
        "id": "rnwzwd1jAC0J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "init_epoch_loss, init_epoch_acc = evaluate(val_loader,model)\n",
        "init_epoch_loss, init_epoch_acc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OnXEaOYYACpX",
        "outputId": "750aefbb-90ce-4f01-a8aa-baad206b06e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(7.1358, device='cuda:0'), tensor(0.0003))"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def fit(model, epoches, lr, train_loader, val_loader, opt=torch.optim.SGD):\n",
        "  optimizer = torch.optim.SGD(model.parameters(), lr, momentum=0.9, weight_decay=5e-4 )\n",
        "  \n",
        "  \n",
        "  history = []\n",
        "\n",
        "\n",
        "  for epoch in range(epoches):\n",
        "    \n",
        "    \n",
        "    for images, labels in train_loader:\n",
        "      model.train()\n",
        "      output = model(images)\n",
        "      \n",
        "      loss = F.cross_entropy(output, labels)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "    epoch_loss, epoch_acc = evaluate(val_loader,model)\n",
        "    print(\"Epoch [{}], epoch_loss: {:.4f}, epoch_acc: {:.4f}\".format(epoch, epoch_loss, epoch_acc))\n",
        "    history.append({'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}) \n",
        "\n"
      ],
      "metadata": {
        "id": "QdbgpwYD6kd5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = fit(model, 10, 0.001, train_loader, val_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2APm1JpvBoE9",
        "outputId": "d370aa10-8ce3-4eb9-f04f-8ea45e702bbf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [0], epoch_loss: 4.7031, epoch_acc: 0.0604\n",
            "Epoch [1], epoch_loss: 4.4396, epoch_acc: 0.0863\n",
            "Epoch [2], epoch_loss: 4.2979, epoch_acc: 0.1075\n",
            "Epoch [3], epoch_loss: 4.2472, epoch_acc: 0.1116\n",
            "Epoch [4], epoch_loss: 4.1476, epoch_acc: 0.1242\n",
            "Epoch [5], epoch_loss: 4.0939, epoch_acc: 0.1309\n",
            "Epoch [6], epoch_loss: 4.0155, epoch_acc: 0.1418\n",
            "Epoch [7], epoch_loss: 4.0139, epoch_acc: 0.1451\n",
            "Epoch [8], epoch_loss: 3.9627, epoch_acc: 0.1536\n",
            "Epoch [9], epoch_loss: 3.9346, epoch_acc: 0.1569\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "overall_history = history\n",
        "accuracies = [result['val_acc'] for result in overall_history] \n",
        "plt.plot(accuracies, '-x')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('accuracy')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "id": "ExP_JcKRczIu",
        "outputId": "479cac38-2c42-4eaa-e095-5e95d1eb48b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-47-cb656982c320>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0moverall_history\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0maccuracies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_acc'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moverall_history\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'-x'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'epoch'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
          ]
        }
      ]
    }
  ]
}